{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sounddevice\n",
      "  Downloading sounddevice-0.4.4-py3-none-win_amd64.whl (195 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\saide\\anaconda3\\lib\\site-packages (from sounddevice) (1.14.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\saide\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.20)\n",
      "Installing collected packages: sounddevice\n",
      "Successfully installed sounddevice-0.4.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171035 sha256=2de9656fceba645a4eb9b8598ce4ca48a8dd2353ec1a1507889d6c62c8fb25fa\n",
      "  Stored in directory: c:\\users\\saide\\appdata\\local\\pip\\cache\\wheels\\5e\\8c\\80\\c3646df8201ba6f5070297fe3779a4b70265d0bfd961c15302\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-1.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing the Libraries</h1>\n",
    "\n",
    "At first, let's import all the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa as rosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sounddevice as sd\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing the Trained Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RNN model from the h5 file \n",
    "rnn_h5 = tf.keras.models.load_model('RNN_RAVDESS.h5')\n",
    "\n",
    "# Load the arrays containing means and standard deviations of features from training for the RNN model\n",
    "mean_X = np.load('mean_X.npy')\n",
    "std_X = np.load('std_X.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Setting the Audio Parameters</h1>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 16000  # Record at 16000 samples per second\n",
    "median_num_frames = 230  # From training data\n",
    "seconds = 7.36  # Length of recording (230*512/16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Defining the String Labels</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_label(argument):\n",
    "    switcher = {\n",
    "        1:\"Neutral\",\n",
    "        2:\"Happy\",\n",
    "        3:\"Sad\",\n",
    "        4:\"Angry\",\n",
    "        5:\"Fearful\",\n",
    "        6:\"Disgust\",\n",
    "        7:\"Surprised\"\n",
    "    }\n",
    "    return switcher.get(argument, \"Nothing\")\n",
    "\n",
    "def label_emoji(argument):\n",
    "    switcher = {\n",
    "        1:emoji.emojize(\":neutral_face:\"),\n",
    "        2:emoji.emojize(\":grinning_face_with_smiling_eyes:\"),\n",
    "        3:emoji.emojize(\":disappointed_face:\"),\n",
    "        4:emoji.emojize(\":angry_face:\"),\n",
    "        5:emoji.emojize(\":fearful_face:\"),\n",
    "        6:emoji.emojize(\":face_vomiting:\"),\n",
    "        7:emoji.emojize(\":hushed_face:\"),\n",
    "    }\n",
    "    return switcher.get(argument, \"Nothing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Recording Audio and making Predictions</h1>\n",
    "\n",
    "This is the cell which will perform the actual audio recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "ðŸ¤®  :  Disgust\n",
      "ðŸ¤®  :  Disgust\n",
      "ðŸ¤®  :  Disgust\n",
      "ðŸ¤®  :  Disgust\n",
      "Recording has ended!\n"
     ]
    }
   ],
   "source": [
    "print('Recording...')\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # In sounddevice, frames mean samples!\n",
    "        # Blocksize is the number of samples per frame!\n",
    "\n",
    "        # Store recorded signal into a Numpy array.\n",
    "        sig = sd.rec(frames=int(fs*seconds), samplerate=fs, channels=1, blocksize=512)\n",
    "\n",
    "        sd.wait() # Wait until recording is finished\n",
    "\n",
    "        sig = np.reshape(sig, (117760,))    # 16000 Hz * 7.36 seconds\n",
    "\n",
    "\n",
    "        # RNN feature extraction\n",
    "        # 'rosa.feature.mfcc' extracts n_mfccs from signal and stores it into 'mfcc_feat'.\n",
    "        mfcc_feat = rosa.feature.mfcc(y=sig, sr=fs, n_mfcc=26, n_fft=512, hop_length=256, htk=True)\n",
    "\n",
    "        spec_feat = rosa.feature.spectral_contrast(y=sig, sr=fs, n_fft=512, hop_length=256)\n",
    "\n",
    "        poly_feat = rosa.feature.poly_features(y=sig, sr=fs, n_fft=512, hop_length=256)\n",
    "\n",
    "        rms_feat = rosa.feature.rms(y=sig, frame_length=512, hop_length=256)\n",
    "\n",
    "        # Append the three 1D arrays into a single 1D array called 'feat'.\n",
    "        feat0 = np.append(mfcc_feat, spec_feat, axis=0)\n",
    "\n",
    "        feat1 = np.append(feat0, poly_feat, axis=0)\n",
    "\n",
    "        feat2 = np.append(feat1, rms_feat, axis=0)\n",
    "        \n",
    "        # Transpose the array to flip the rows and columns. This is done so that the features become column parameters, making each row an audio frame.\n",
    "        transp_feat = feat2.T\n",
    "\n",
    "        # Note: The 'cap frame number' is basically the limit we set for the number of frames for each audio file, so that all audio files have equal lengths when processing.\n",
    "        \n",
    "        if transp_feat.shape[0] < median_num_frames:\n",
    "            # If number of frames is smaller than the cap frame number, we pad the array in order to reach our desired dimensions.\n",
    "            # Pad the array so that it matches the cap frame number. The second value in the argument contains two tuples which indicate which way to pad how much.  \n",
    "            transp_feat = np.pad(transp_feat, ((0, median_num_frames-transp_feat.shape[0]), (0,0)), constant_values=0)\n",
    "\n",
    "        elif transp_feat.shape[0] > median_num_frames:\n",
    "            # If number of frames is larger than the cap frame number, we delete rows (frames) which exceed the cap frame number in order to reach our desired dimensions.\n",
    "            # Define a tuple which contains the range of the row indices to delete.\n",
    "            row_del_index = (range(median_num_frames, transp_feat.shape[0], 1))\n",
    "            transp_feat = np.delete(transp_feat, row_del_index, axis=0)\n",
    "\n",
    "        else:\n",
    "            # If number of frames match the cap frame length, perfect!\n",
    "            transp_feat = transp_feat\n",
    "\n",
    "        # Transpose again to flip the rows and columns. This is done so that the features become row parameters, making each column an audio frame.\n",
    "        transp2_feat = transp_feat.T\n",
    "\n",
    "        # Flatten the entire 2D Numpy array into 1D Numpy array. So, the first 36 values of the 1D array represent the features for first frame, the second 36 represent the features for second frame, and so on till the final (cap) frame.\n",
    "        # 'C' means row-major ordered flattening.\n",
    "        feat_rnn = transp2_feat.flatten('C')\n",
    "\n",
    "        feat_rnn = np.reshape(feat_rnn, (1,-1)) \n",
    "\n",
    "        # Standardize the inputs means and standard deviations of features from training for RNN model.\n",
    "        feat_centered_rnn = (feat_rnn - mean_X)/std_X\n",
    "\n",
    "        # Reshaping feat_centered to 3D Numpy array for feeding into the RNN. RNNs require 3D array input.\n",
    "        # 3D dimensions are (layers, rows, columns).\n",
    "        feat_3D = np.reshape(feat_centered_rnn, (feat_centered_rnn.shape[0], median_num_frames, 36))\n",
    "\n",
    "        # Transpose tensors so that rows=features and columns=frames.\n",
    "        feat_3D_posed = tf.transpose(feat_3D, perm=[0, 2, 1])\n",
    "\n",
    "        # Make prediction using RNN model.\n",
    "        pred = rnn_h5.predict(feat_3D_posed)\n",
    "\n",
    "        # Convert One Hot label to integer label.\n",
    "        pred = int(np.argmax(pred, axis=1))\n",
    "        \n",
    "        # Get the corresponding string label.\n",
    "        emotion = change_label(pred)\n",
    "        \n",
    "        # Get the corresponding emoji.\n",
    "        smiley = label_emoji(pred)\n",
    "        \n",
    "        # Print the output.\n",
    "        print(smiley, \" : \", emotion)\n",
    "        \n",
    "        del sig\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('Recording has ended!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
